---
title: August 10
date: 2024-08-10
cover: https://oss.justin3go.com/blogs/fav0-001.jpg
---

Spend 1 minute each day to get curated cutting-edge AI information.

The content includes but is not limited to **cutting-edge AI news**, **AI tools**, **AI art**, **open-source projects**, and **learning tutorials**, etc.

Follow the AI Daily to keep up with the AI trend, and I hope it will be helpful to you. For important information, separate posts will be made for detailed introductions.

Below is the latest AI information for August 10th.

### Cutting-Edge News

**1. Alibaba's audio model Qwen2-Audio releases an online demo.**

You can interact with the model through voice input, and the model will analyze and interpret various types of audio information based on voice commands.

Online demo: https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo

Currently, it supports 8 languages including Chinese, Cantonese, English, French, Japanese, etc. I was quite surprised to see that Cantonese is supported.

![image-20240810225626282](https://cdn.jsdelivr.net/gh/freelander/oss@master/ai-daily/2024-08-10/image-20240810225626282.png)

### AI Art

**1. Google's image generation model Imagen 3 is now open to everyone!**

It supports partial re-drawing, allowing you to edit images with brushes and prompts.

Usage link: https://aitestkitchen.withgoogle.com/zh/tools/image-fx

Note that it is temporarily unavailable in some regions; I have personally confirmed that it works in Japan.

![image-20240810232634613](https://cdn.jsdelivr.net/gh/freelander/oss@master/ai-daily/2024-08-10/image-20240810232634613.png)

**2. The FLUX product for image generation with ControlNet is now available for online experience!**

Experience link: https://huggingface.co/spaces/DamarJati/FLUX.1-DEV-Canny

Currently, only the Canny mode of ControlNet is supported.

![image-20240810233658913](https://cdn.jsdelivr.net/gh/freelander/oss@master/ai-daily/2024-08-10/image-20240810233658913.png)

### Learning Tools

**1. An interactive learning tool for Transformer visualization: Transformer Explainer.**

Using GPT-2 as an example, you can input text to observe in real-time how each component (embedding layer, self-attention mechanism, MLP, etc.) processes the text and predicts the next token.

GitHub: https://github.com/poloclub/transformer-explainer

Learning link: https://poloclub.github.io/transformer-explainer/

This tool, which combines visualization and interaction to help users understand the internal components of Transformers, is especially suitable for teaching. It's very illustrative, so if you're interested, give it a try.

![image-20240810231703834](https://cdn.jsdelivr.net/gh/freelander/oss@master/ai-daily/2024-08-10/image-20240810231703834.png)
